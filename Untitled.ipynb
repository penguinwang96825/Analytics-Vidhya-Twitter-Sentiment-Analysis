{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_df['df_type'] = 'train'\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df['df_type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id         0\n",
      "label      0\n",
      "tweet      0\n",
      "df_type    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Concatenating test and train data\n",
    "combined_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Check null values\n",
    "print(train_df.apply(lambda x: sum(x.isnull())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hash_tags(text):\n",
    "    return list(set(part[1:] for part in text.split() if part.startswith('#')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n",
      "Extract hash tags: {'run'}\n"
     ]
    }
   ],
   "source": [
    "sample = train_df.loc[0, \"tweet\"]\n",
    "print(\"Original sentence: {}\".format(sample))\n",
    "print(\"Extract hash tags: {}\".format(extract_hash_tags(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference from https://stackoverflow.com/a/38119388/15048366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def clean_tweets(text):\n",
    "    text = strip_links(text)\n",
    "    text = strip_all_entities(text)\n",
    "    return text\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "def strip_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "def strip_links(text):\n",
    "    link_regex = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], '[LINK]')    \n",
    "    return text\n",
    "\n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@', '#']\n",
    "    for separator in string.punctuation.replace(\"[\", \"\").replace(\"]\", \"\"):\n",
    "        if separator not in entity_prefixes:\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: \n",
      "\t@peter I really love that shirt at #Macy. http://bit.ly//WjdiW4\n",
      "Cleaned tweet: \n",
      "\tI really love that shirt at [LINK]\n",
      "Hash tags: \n",
      "\t['Macy.']\n"
     ]
    }
   ],
   "source": [
    "sample = \"@peter I really love that shirt at #Macy. http://bit.ly//WjdiW4\"\n",
    "\n",
    "print(\"Original tweet: \\n\\t{}\".format(sample))\n",
    "print(\"Cleaned tweet: \\n\\t{}\".format(clean_tweets(sample)))\n",
    "print(\"Hash tags: \\n\\t{}\".format(extract_hash_tags(sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"tweet_clean\"] = train_df[\"tweet\"].apply(clean_tweets)\n",
    "train_df[\"tweet_tokenised\"] = train_df[\"tweet_clean\"].apply(tokenization)\n",
    "train_df[\"tweet_stem\"] = train_df[\"tweet_tokenised\"].apply(stemming)\n",
    "train_df[\"tweet_lemm\"] = train_df[\"tweet_tokenised\"].apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v_model = KeyedVectors.load_word2vec_format(r\"G:\\kaggle\\embedding\\GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = word2vec.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec, use_idf=True):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.use_idf = use_idf\n",
    "        self.dim = word2vec.vector_size\n",
    "        \n",
    "    def word2tf(self, term_list):\n",
    "        term_freq = Counter(term_list)\n",
    "        total_len = sum(term_freq.values())\n",
    "        term_freq = [(term, term_freq[term]/total_len) for term, count in term_freq.items()]\n",
    "        return dict(term_freq)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for doc in X:\n",
    "            weighted_array = []\n",
    "            for term in doc:\n",
    "                if term in self.word2vec:\n",
    "                    if self.use_idf:\n",
    "                        weighted_term = self.word2vec[term] * self.word2tf(doc)[term] * self.word2weight[term]\n",
    "                    else:\n",
    "                        weighted_term = self.word2vec[term] * self.word2tf(doc)[term]\n",
    "                    weighted_array.append(weighted_term)\n",
    "            weighted_array = np.mean(weighted_array or [np.zeros(self.dim)], axis=0)\n",
    "            transformed_X.append(weighted_array)\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "def ml_pipeline_with_embedding(clf, \n",
    "                               w2v_gensim, \n",
    "                               search_spaces, \n",
    "                               mode=\"mean\", \n",
    "                               cv=5, \n",
    "                               refit=True, \n",
    "                               scoring='roc_auc', \n",
    "                               verbose=1, \n",
    "                               n_jobs=8):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf: scikit-learn estimator\n",
    "        clf is a scikit-learn estimator interface.\n",
    "    w2v_gensim: gensim model\n",
    "        Word2Vec class from gensim.\n",
    "    search_spaces: dict\n",
    "        A dictinary where keys are parameter names (strings) and values are skopt.space.Dimension \n",
    "        instances (Real, Integer or Categorical).\n",
    "    mode: str {\"mean\", \"tfidf\"}\n",
    "        MeanEmbeddingVectorizer() or TfidfEmbeddingVectorizer() as embedding method.\n",
    "    cv: int\n",
    "        A integer, to specify the number of folds in a (Stratified) KFold.\n",
    "    refit: boolean\n",
    "        Refit the best estimator with the entire dataset. If “False”, it is impossible to make \n",
    "        predictions using this RandomizedSearchCV instance after fitting.\n",
    "    scoring: str\n",
    "        Defining model evaluation rules.\n",
    "        https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "    verbose: int\n",
    "        Controls the verbosity: the higher, the more messages.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    [1] https://www.kaggle.com/nanomathias/bayesian-optimization-of-xgboost-lb-0-9769\n",
    "    [2] https://stackoverflow.com/q/64264724/15048366\n",
    "    \"\"\"\n",
    "    if mode == \"mean\":\n",
    "        model = Pipeline([\n",
    "            (\"word2vec vectorizer (mean)\", MeanEmbeddingVectorizer(w2v_gensim)), \n",
    "            (\"machine learing classifier\", BayesSearchCV(estimator=clf,\n",
    "                                                         search_spaces=search_spaces,\n",
    "                                                         cv=StratifiedKFold(\n",
    "                                                             n_splits=cv,\n",
    "                                                             shuffle=True,\n",
    "                                                             random_state=42), \n",
    "                                                         refit=True, \n",
    "                                                         scoring=scoring, \n",
    "                                                         random_state=42, \n",
    "                                                         n_jobs=n_jobs))], verbose=verbose)\n",
    "    elif mode == \"tfidf\":\n",
    "        model = Pipeline([\n",
    "            (\"word2vec vectorizer (tfidf)\", TfidfEmbeddingVectorizer(w2v_gensim)), \n",
    "            (\"machine learing classifier\", BayesSearchCV(estimator=clf,\n",
    "                                                         search_spaces=search_spaces,\n",
    "                                                         cv=StratifiedKFold(\n",
    "                                                             n_splits=cv,\n",
    "                                                             shuffle=True,\n",
    "                                                             random_state=42), \n",
    "                                                         refit=True, \n",
    "                                                         scoring=scoring, \n",
    "                                                         random_state=42, \n",
    "                                                         n_jobs=n_jobs))], verbose=verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 1 of 2) Processing word2vec vectorizer (tfidf), total=   6.4s\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = XGBClassifier()\n",
    "search_spaces = {\n",
    "    'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "    'min_child_weight': Integer(0, 10),\n",
    "    'max_depth': Integer(0, 50),\n",
    "    'max_delta_step': Integer(0, 20),\n",
    "    'subsample': Real(0.01, 1.0, 'uniform'),\n",
    "    'colsample_bytree': Real(0.01, 1.0, 'uniform'),\n",
    "    'colsample_bylevel': Real(0.01, 1.0, 'uniform'),\n",
    "    'reg_lambda': Real(1e-9, 1000, 'log-uniform'),\n",
    "    'reg_alpha': Real(1e-9, 1.0, 'log-uniform'),\n",
    "    'gamma': Real(1e-9, 0.5, 'log-uniform'),\n",
    "    'min_child_weight': Integer(0, 5),\n",
    "    'n_estimators': Integer(100, 500),\n",
    "    'scale_pos_weight': Real(1e-6, 500, 'log-uniform')\n",
    "}\n",
    "xgb_model = ml_pipeline_with_embedding(xgb_clf, w2v_model, search_spaces, mode=\"tfidf\", cv=3, scoring=\"f1\")\n",
    "xgb_model.fit(train_df[\"tweet_tokenised\"], train_df[\"label\"])\n",
    "print(\"Done training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_test = xgb_model.predict_proba(train_df[\"tweet_tokenised\"])\n",
    "pred_test = np.argmax(proba_test, axis=1)\n",
    "print(\"F1: {:.4f}\\nAcc: {:.4f}\".format(f1_score(train_df[\"label\"], pred_test), \n",
    "                                       accuracy_score(train_df[\"label\"], pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "test_df[\"tweet_clean\"] = test_df[\"tweet\"].apply(clean_tweets)\n",
    "test_df[\"tweet_tokenised\"] = test_df[\"tweet_clean\"].apply(tokenization)\n",
    "proba_test = xgb_model.predict_proba(test_df[\"tweet_tokenised\"])\n",
    "pred_test = np.argmax(proba_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 1 of 2) Processing word2vec vectorizer (tfidf), total=   0.1s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-218-5dcd002e9b9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mlgb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mml_pipeline_with_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBMClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_spaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tfidf\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"f1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mlgb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tweet_tokenised\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\skopt\\searchcv.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, callback)\u001b[0m\n\u001b[0;32m    692\u001b[0m                 optim_result = self._step(\n\u001b[0;32m    693\u001b[0m                     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m                     \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_points_adjusted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m                 )\n\u001b[0;32m    696\u001b[0m                 \u001b[0mn_iter\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mn_points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\skopt\\searchcv.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, X, y, search_space, optimizer, groups, n_points)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[1;31m# get parameter values to evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;31m# convert parameters to python native types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\skopt\\optimizer\\optimizer.py\u001b[0m in \u001b[0;36mask\u001b[1;34m(self, n_points, strategy)\u001b[0m\n\u001b[0;32m    415\u001b[0m                 \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_lie\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_lie\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m                 \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_lie\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_points\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m}\u001b[0m  \u001b[1;31m# cache_ the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\skopt\\optimizer\\optimizer.py\u001b[0m in \u001b[0;36m_tell\u001b[1;34m(self, x, y, fit)\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m                 \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"next_xs_\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macq_func\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"gp_hedge\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\skopt\\learning\\gaussian_process\\gpr.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mnoise_level\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_level_bounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"fixed\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             )\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGaussianProcessRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoise_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    232\u001b[0m             optima = [(self._constrained_optimization(obj_func,\n\u001b[0;32m    233\u001b[0m                                                       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m                                                       self.kernel_.bounds))]\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;31m# Additional runs are performed from log-uniform chosen initial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\u001b[0m in \u001b[0;36m_constrained_optimization\u001b[1;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[0;32m    501\u001b[0m             opt_res = scipy.optimize.minimize(\n\u001b[0;32m    502\u001b[0m                 \u001b[0mobj_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_theta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"L-BFGS-B\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m                 bounds=bounds)\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[0m_check_optimize_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lbfgs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[0mtheta_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m--> 618\u001b[1;33m                                 callback=callback, **options)\n\u001b[0m\u001b[0;32m    619\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    306\u001b[0m     sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n\u001b[0;32m    307\u001b[0m                                   \u001b[0mbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_bounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                                   finite_diff_rel_step=finite_diff_rel_step)\n\u001b[0m\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[0mfunc_and_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun_and_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[1;32m--> 262\u001b[1;33m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# Gradient evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\u001b[0m in \u001b[0;36mobj_func\u001b[1;34m(theta, eval_gradient)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                     lml, grad = self.log_marginal_likelihood(\n\u001b[1;32m--> 225\u001b[1;33m                         theta, eval_gradient=True, clone_kernel=False)\n\u001b[0m\u001b[0;32m    226\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\u001b[0m in \u001b[0;36mlog_marginal_likelihood\u001b[1;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcho_solve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Line 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[1;31m# Compute log-likelihood (compare line 7)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\scipy\\linalg\\decomp_cholesky.py\u001b[0m in \u001b[0;36mcho_solve\u001b[1;34m(c_and_lower, b, overwrite_b, check_finite)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_and_lower\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray_chkfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray_chkfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AllFloat'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         raise ValueError(\n\u001b[1;32m--> 486\u001b[1;33m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[0;32m    487\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_clf = lgb.LGBMRegressor(objective='binary', metric='auc', n_jobs=-1, verbose=0)\n",
    "search_spaces = {\n",
    "    'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "    'num_leaves': Integer(1, 100),      \n",
    "    'max_depth': Integer(0, 50),\n",
    "    'min_child_samples': Integer(0, 50),\n",
    "    'max_bin': Integer(100, 1000),\n",
    "    'subsample': Real(0.01, 1.0, 'uniform'),\n",
    "    'subsample_freq': Integer(0, 10),\n",
    "    'colsample_bytree': Real(0.01, 1.0, 'uniform'),\n",
    "    'min_child_weight': Integer(0, 10),\n",
    "    'subsample_for_bin': Integer(100000, 500000),\n",
    "    'reg_lambda': Real(1e-9, 1000, 'log-uniform'),\n",
    "    'reg_alpha': Real(1e-9, 1.0, 'log-uniform'),\n",
    "    'scale_pos_weight': Real(1e-6, 500, 'log-uniform'),\n",
    "    'n_estimators': Integer(50, 100),\n",
    "}\n",
    "\n",
    "class LGBM(lgb.LGBMClassifier):\n",
    "    \"\"\"\n",
    "    Use a callback function to echo out results after each iteration when optimizing with BayesSearchCV.\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    [1] https://github.com/scikit-optimize/scikit-optimize/issues/698#issuecomment-401059610\n",
    "    \"\"\"\n",
    "    def __init__(self, objective='binary', metric='auc', n_jobs=-1, verbose=0):\n",
    "        super().__init__(objective, metric, n_jobs, verbose)\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        print(params)\n",
    "        super().set_params(**params)\n",
    "\n",
    "lgb_model = ml_pipeline_with_embedding(lgb_clf, w2v_model, search_spaces, mode=\"tfidf\", cv=3, scoring=\"f1\")\n",
    "lgb_model.fit(train_df[\"tweet_tokenised\"][:100], train_df[\"label\"][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Wrapper\n",
    "\n",
    "## Reference\n",
    "1. https://www.kaggle.com/graymant/pytorch-regression-with-sklearn-pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class PytorchClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"A pytorch classifier\"\"\"\n",
    "    def __init__(self, \n",
    "                 num_epochs=1, \n",
    "                 learning_rate=0.01, \n",
    "                 batch_size=128, \n",
    "                 shuffle=False, \n",
    "                 callbacks=[], \n",
    "                 use_gpu=True, \n",
    "                 verbose=1):\n",
    "        \"\"\"\n",
    "        Called when initializing the regressor\n",
    "        \"\"\"\n",
    "        self._history = None\n",
    "        self._model = None\n",
    "        self._gpu = use_gpu and torch.cuda.is_available()\n",
    "\n",
    "        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n",
    "        values.pop(\"self\")\n",
    "\n",
    "        for arg, val in values.items():\n",
    "            setattr(self, arg, val)\n",
    "\n",
    "    def _build_model(self):\n",
    "        self._layer_dims = [self.input_dim] + \\\n",
    "            self.hidden_layer_dims + [self.output_dim]\n",
    "\n",
    "        self._model = torch.nn.Sequential()\n",
    "\n",
    "        # Loop through the layer dimensions and create an input layer, then\n",
    "        # create each hidden layer with relu activation.\n",
    "        for idx, dim in enumerate(self._layer_dims):\n",
    "            if (idx < len(self._layer_dims) - 1):\n",
    "                module = torch.nn.Linear(dim, self._layer_dims[idx + 1])\n",
    "                init.xavier_uniform(module.weight)\n",
    "                self._model.add_module(\"linear\" + str(idx), module)\n",
    "\n",
    "            if (idx < len(self._layer_dims) - 2):\n",
    "                self._model.add_module(\"relu\" + str(idx), torch.nn.ReLU())\n",
    "\n",
    "        if self._gpu:\n",
    "            self._model = self._model.cuda()\n",
    "\n",
    "    def _train_model(self, X, y):\n",
    "        torch_x = torch.from_numpy(X).float()\n",
    "        torch_y = torch.from_numpy(y).float()\n",
    "        if self._gpu:\n",
    "            torch_x = torch_x.cuda()\n",
    "            torch_y = torch_y.cuda()\n",
    "\n",
    "        train = data_utils.TensorDataset(torch_x, torch_y)\n",
    "        train_loader = data_utils.DataLoader(train, batch_size=self.batch_size,\n",
    "                                             shuffle=self.shuffle)\n",
    "\n",
    "        loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self._model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        self._history = {\"loss\": [], \"val_loss\": [], \"mse_loss\": []}\n",
    "\n",
    "        finish = False\n",
    "        for epoch in range(self.num_epochs):\n",
    "            if finish:\n",
    "                break\n",
    "\n",
    "            loss = None\n",
    "            idx = 0\n",
    "            for idx, (minibatch, target) in enumerate(train_loader):\n",
    "                y_pred = self._model(Variable(minibatch))\n",
    "\n",
    "                loss = loss_fn(y_pred, Variable(\n",
    "                    target.cuda().float() if self._gpu else target.float()))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            y_labels = target.cpu().numpy() if self._gpu else target.numpy()\n",
    "            y_pred_results = y_pred.cpu().data.numpy() if self._gpu else y_pred.data.numpy()\n",
    "\n",
    "            error = mean_absolute_error(y_labels, y_pred_results)\n",
    "\n",
    "            self._history[\"mse_loss\"].append(loss.data[0])\n",
    "            self._history[\"loss\"].append(error)\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print(\"Results for epoch {}, loss {}, mse_loss {}\".format(epoch + 1,\n",
    "                                                                          error, loss.data[0]))\n",
    "            for callback in self.callbacks:\n",
    "                callback.call(self._model, self._history)\n",
    "                if callback.finish:\n",
    "                    finish = True\n",
    "                    break\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the pytorch regressor.\n",
    "        \"\"\"\n",
    "        assert (type(self.input_dim) == int), \"input_dim parameter must be defined\"\n",
    "        assert (type(self.output_dim) == int), \"output_dim must be defined\"\n",
    "\n",
    "        self._build_model()\n",
    "        self._train_model(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Makes a prediction using the trained pytorch model\n",
    "        \"\"\"\n",
    "        if self._history == None:\n",
    "            raise RuntimeError(\"Classifier has not been fit\")\n",
    "\n",
    "        results = []\n",
    "        split_size = math.ceil(len(X) / self.batch_size)\n",
    "\n",
    "        # In case the requested size of prediction is too large for memory (especially gpu)\n",
    "        # split into batchs, roughly similar to the original training batch size. Not\n",
    "        # particularly scientific but should always be small enough.\n",
    "        for batch in np.array_split(X, split_size):\n",
    "            x_pred = Variable(torch.from_numpy(batch).float())\n",
    "            y_pred = self._model(x_pred.cuda() if self._gpu else x_pred)\n",
    "            y_pred_formatted = y_pred.cpu().data.numpy() if self._gpu else y_pred.data.numpy()\n",
    "            results = np.append(results, y_pred_formatted)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Scores the data using the trained pytorch model. Under current implementation\n",
    "        returns negative mae.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X, y)\n",
    "        return mean_absolute_error(y, y_pred) * -1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
